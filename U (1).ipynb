{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTIbG8jUuF3O"
   },
   "source": [
    "## Problem: \n",
    "\n",
    "Seismic data is collected using reflection seismology, or seismic reflection. The method requires a controlled seismic source of energy, such as compressed air or a seismic vibrator, and sensors record the reflection from rock interfaces within the subsurface. The recorded data is then processed to create a 3D view of earth’s interior. Reflection seismology is similar to X-ray, sonar and echolocation.\n",
    "\n",
    "A seismic image is produced from imaging the reflection coming from rock boundaries. The seismic image shows the boundaries between different rock types. In theory, the strength of reflection is directly proportional to the difference in the physical properties on either sides of the interface. While seismic images show rock boundaries, they don't say much about the rock themselves; some rocks are easy to identify while some are difficult.\n",
    "\n",
    "There are several areas of the world where there are vast quantities of salt in the subsurface. One of the challenges of seismic imaging is to identify the part of subsurface which is salt. Salt has characteristics that makes it both simple and hard to identify. Salt density is usually 2.14 g/cc which is lower than most surrounding rocks. The seismic velocity of salt is 4.5 km/sec, which is usually faster than its surrounding rocks. This difference creates a sharp reflection at the salt-sediment interface. Usually salt is an amorphous rock without much internal structure. This means that there is typically not much reflectivity inside the salt, unless there are sediments trapped inside it. The unusually high seismic velocity of salt can create problems with seismic imaging.\n",
    "\n",
    "### Data\n",
    "The data is a set of images chosen at various locations chosen at random in the subsurface. The images are 101 x 101 pixels and each pixel is classified as either salt or sediment. In addition to the seismic images, the depth of the imaged location is provided for each image. The goal of the competition is to segment regions that contain salt.\n",
    "\n",
    "#### Source: \n",
    "https://www.kaggle.com/c/tgs-salt-identification-challenge\n",
    "\n",
    "\n",
    "### Note: \n",
    "Accept the terms and download data from the above link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMf_X7vKug7M"
   },
   "source": [
    "### Aim: \n",
    "\n",
    "Implement U-Net neural model architecture in keras to solve this problem.\n",
    "\n",
    "\n",
    "In this, you are asked to segment salt deposits beneath the Earth’s surface. Given a set of seismic images that are 101 x 101 pixels each and each pixel we need to classify as either salt or sediment. Our goal is to segment regions that contain salt. A seismic image is produced from imaging the reflection coming from rock boundaries. The seismic image shows the boundaries between different rock types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nradNFo3vUTo"
   },
   "source": [
    "### Steps:\n",
    "\n",
    "1. Download the dataset\n",
    "2. Upload to Drive\n",
    "3. Import from drive to colab\n",
    "4. Load the images\n",
    "5. Build U-net Model\n",
    "6. Train\n",
    "7. Report train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2109,
     "status": "error",
     "timestamp": 1549849499062,
     "user": {
      "displayName": "Sountharya Divya",
      "photoUrl": "https://lh4.googleusercontent.com/-9yZ1pK_A2Vw/AAAAAAAAAAI/AAAAAAAAAJ0/s7ar3k3pccY/s64/photo.jpg",
      "userId": "00130442249997131471"
     },
     "user_tz": 480
    },
    "id": "khDX1s6wtxR6",
    "outputId": "3ccd4864-169e-4b26-99d8-3579cb681c0e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/content/drive/My drive/colab notebook/train/mask/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fd304997b6c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mTEST_IMAGE_DIR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../input/test/images/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_IMAGE_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/drive/My drive/colab notebook/train/mask/'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm #Progress bar\n",
    "import os\n",
    "TRAIN_IMAGE_DIR = '/content/drive/My drive/colab notebook/train/images/' #img_id is x(input)\n",
    "TRAIN_MASK_DIR = '/content/drive/My drive/colab notebook/train/mask/'   #rle_mask is y(output)\n",
    "TEST_IMAGE_DIR = '/content/drive/My drive/colab notebook/train/images/'\n",
    "\n",
    "train_d = os.listdir(TRAIN_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMB2__TEJqaa"
   },
   "outputs": [],
   "source": [
    "x = [np.array(cv2.imread(TRAIN_IMAGE_DIR + p, cv2.IMREAD_GRAYSCALE), dtype=np.uint8) for p in tqdm(train_d)] #cv2.imread=openCV image read\n",
    "x = np.array(x)/255\n",
    "\n",
    "y = [np.array(cv2.imread(TRAIN_MASK_DIR + p, cv2.IMREAD_GRAYSCALE), dtype=np.uint8) for p in tqdm(train_d)]\n",
    "y = np.array(y)/255\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmEl-OnZJwKi"
   },
   "outputs": [],
   "source": [
    "x=np.expand_dims(x,axis=3) #EXPAND DIM OF X AND INSERT NEW AXIS @ 3 \n",
    "y=np.expand_dims(y,axis=3)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eb-VFABDJ7Bt"
   },
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D,Conv2D,Dense,Dropout,Input,Conv2DTranspose,Concatenate\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "def conv_block(num_layers,inp,units,kernel_size):\n",
    "    x = input\n",
    "    for l in range(num_layers): #repeat 32-24-16 ----4 times\n",
    "        x = Conv2D(units, kernel_size=kernel_size,padding='SAME',activation='relu')(x)\n",
    "    return x\n",
    "input = Input(shape=(101,101,1))\n",
    "cnn1 = conv_block(5,input,32,3)\n",
    "cnn2 = conv_block(5,input,24,5)\n",
    "cnn3 = conv_block(5,input,16,7)\n",
    "cnn4 = conv_block(5,input,8,9)\n",
    "cnn5 = conv_block(5,input,4,11)\n",
    "concat = Concatenate()([cnn1,cnn2,cnn3,cnn4,cnn5])\n",
    "\n",
    "d1 = Conv2D(16,1,activation='relu')(concat)\n",
    "out = Conv2D(1,1,activation='sigmoid')(d1) #filter_size = 1 ,so that 1x1 filter will scan over for more learning\n",
    "\n",
    "model = Model(inputs=[input], outputs=[out])\n",
    "adam=Adam(lr=0.001)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "model.summary() # start_dim=(101,101,1) == #end_dim=(101,101,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxGSm60sJ68-"
   },
   "outputs": [],
   "source": [
    "keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "model.fit(x,y,epochs=50,batch_size=128,validation_split=0.2,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAqd5wbSJ6Rp"
   },
   "outputs": [],
   "source": [
    "#test_data\n",
    "test_d=os.listdir(TEST_IMAGE_DIR)\n",
    "\n",
    "x_test = [np.array(cv2.imread(TEST_IMAGE_DIR + p, cv2.IMREAD_GRAYSCALE), dtype=np.uint8) for p in tqdm(test_d)]\n",
    "x_test = np.array(x_test)/255\n",
    "print(x_test.shape)\n",
    "x_test = np.expand_dims(x_test,axis=3)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjmSkYEQKKZo"
   },
   "outputs": [],
   "source": [
    "predict=model.predict(x_test,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vHOH9eUKKgi"
   },
   "outputs": [],
   "source": [
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "            \n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs\n",
    "\n",
    "pred_dict = {fn[:-4]:RLenc(np.round(predict[i,:,:,0])) for i,fn in tqdm(enumerate(test_d))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vq1bbNg3Klax"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "U_NET_Lab_Questions.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
